{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "wv_pretrained = api.load(\"word2vec-google-news-300\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118191123008728),\n",
       " ('monarch', 0.6189674735069275),\n",
       " ('princess', 0.5902430415153503),\n",
       " ('crown_prince', 0.5499458909034729),\n",
       " ('prince', 0.5377322435379028),\n",
       " ('kings', 0.5236843824386597),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.5181134939193726),\n",
       " ('sultan', 0.5098593235015869),\n",
       " ('monarchy', 0.5087411403656006)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_pretrained.most_similar(positive=[\"king\",\"woman\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analogy Results:\n",
      "king - man + woman ≈ queen\n",
      "('queen', 0.7118191123008728)\n",
      "brother - man + woman ≈ sister\n",
      "('sister', 0.8103213906288147)\n",
      "uncle - man + woman ≈ aunt\n",
      "('aunt', 0.8022664785385132)\n",
      "nephew - man + woman ≈ niece\n",
      "('niece', 0.8202236294746399)\n",
      "actor - man + woman ≈ actress\n",
      "('actress', 0.860262393951416)\n",
      "hero - man + woman ≈ heroine\n",
      "('heroine', 0.68734210729599)\n",
      "doctor - hospital + school ≈ guidance_counselor\n",
      "('guidance_counselor', 0.5969594717025757)\n",
      "painter - canvas + stage ≈ cabaret_performer\n",
      "('cabaret_performer', 0.4397791028022766)\n",
      "pilot - airplane + ship ≈ ships\n",
      "('ships', 0.4948738217353821)\n",
      "chef - kitchen + laboratory ≈ lab\n",
      "('lab', 0.5366887450218201)\n",
      "author - book + song ≈ anthem\n",
      "('anthem', 0.5882688164710999)\n",
      "hitler - Germany + India ≈ gandhi\n",
      "('gandhi', 0.619478166103363)\n"
     ]
    }
   ],
   "source": [
    "def perform_analogy(word1, word2, word3):\n",
    "    result = wv_pretrained.most_similar(positive=[word1, word3], negative=[word2])\n",
    "    return result[0]\n",
    "\n",
    "analogies = [\n",
    "    (\"king\", \"man\", \"woman\"),  \n",
    "    (\"brother\", \"man\", \"woman\"),  \n",
    "    (\"uncle\", \"man\", \"woman\"),  \n",
    "    (\"nephew\", \"man\", \"woman\"),  \n",
    "    (\"actor\", \"man\", \"woman\"),  \n",
    "    (\"hero\", \"man\", \"woman\") ,\n",
    "    (\"doctor\", \"hospital\", \"school\"),  # doctor - hospital + school ≈ teacher\n",
    "    (\"painter\", \"canvas\", \"stage\"),  # painter - canvas + stage ≈ actor\n",
    "    (\"pilot\", \"airplane\", \"ship\"),  # pilot - airplane + ship ≈ captain\n",
    "    (\"chef\", \"kitchen\", \"laboratory\"),  # chef - kitchen + laboratory ≈ scientist\n",
    "    (\"author\", \"book\", \"song\"),  # author - book + song ≈ singer\n",
    "    (\"hitler\",\"Germany\", \"India\")\n",
    "]\n",
    "\n",
    "analogy_results = {analogy: perform_analogy(*analogy) for analogy in analogies}\n",
    "\n",
    "print(\"Analogy Results:\")\n",
    "for analogy, result in analogy_results.items():\n",
    "    print(f\"{analogy[0]} - {analogy[1]} + {analogy[2]} ≈ {result[0]}\")\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Task 2 ()\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[One, reviewers, mentioned, watching, 1, Oz, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[A, wonderful, little, production, br, br, The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[I, thought, wonderful, way, spend, time, hot,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[Basically, theres, family, little, boy, Jake,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[Petter, Matteis, Love, Time, Money, visually,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[I, thought, movie, right, good, job, It, wasn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[Bad, plot, bad, dialogue, bad, acting, idioti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[I, Catholic, taught, parochial, elementary, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[Im, going, disagree, previous, comment, side,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[No, one, expects, Star, Trek, movies, high, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment  \\\n",
       "0      One of the other reviewers has mentioned that ...  positive   \n",
       "1      A wonderful little production. <br /><br />The...  positive   \n",
       "2      I thought this was a wonderful way to spend ti...  positive   \n",
       "3      Basically there's a family where a little boy ...  negative   \n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "...                                                  ...       ...   \n",
       "49995  I thought this movie did a down right good job...  positive   \n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative   \n",
       "49997  I am a Catholic taught in parochial elementary...  negative   \n",
       "49998  I'm going to have to disagree with the previou...  negative   \n",
       "49999  No one expects the Star Trek movies to be high...  negative   \n",
       "\n",
       "                                            cleaned_text  \n",
       "0      [One, reviewers, mentioned, watching, 1, Oz, e...  \n",
       "1      [A, wonderful, little, production, br, br, The...  \n",
       "2      [I, thought, wonderful, way, spend, time, hot,...  \n",
       "3      [Basically, theres, family, little, boy, Jake,...  \n",
       "4      [Petter, Matteis, Love, Time, Money, visually,...  \n",
       "...                                                  ...  \n",
       "49995  [I, thought, movie, right, good, job, It, wasn...  \n",
       "49996  [Bad, plot, bad, dialogue, bad, acting, idioti...  \n",
       "49997  [I, Catholic, taught, parochial, elementary, s...  \n",
       "49998  [Im, going, disagree, previous, comment, side,...  \n",
       "49999  [No, one, expects, Star, Trek, movies, high, a...  \n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords_and_punctuation(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "df['cleaned_text'] = df['review'].apply(remove_stopwords_and_punctuation)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050317</td>\n",
       "      <td>0.053415</td>\n",
       "      <td>0.038032</td>\n",
       "      <td>0.076007</td>\n",
       "      <td>-0.053392</td>\n",
       "      <td>-0.002179</td>\n",
       "      <td>0.034439</td>\n",
       "      <td>-0.057433</td>\n",
       "      <td>0.076233</td>\n",
       "      <td>0.097119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065795</td>\n",
       "      <td>0.010038</td>\n",
       "      <td>-0.116087</td>\n",
       "      <td>0.017102</td>\n",
       "      <td>-0.040221</td>\n",
       "      <td>-0.023342</td>\n",
       "      <td>0.002918</td>\n",
       "      <td>-0.075081</td>\n",
       "      <td>0.041535</td>\n",
       "      <td>-0.019835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.066517</td>\n",
       "      <td>0.071847</td>\n",
       "      <td>-0.025412</td>\n",
       "      <td>0.046563</td>\n",
       "      <td>-0.047924</td>\n",
       "      <td>0.037785</td>\n",
       "      <td>0.033784</td>\n",
       "      <td>-0.072095</td>\n",
       "      <td>0.089296</td>\n",
       "      <td>0.087283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133700</td>\n",
       "      <td>0.024485</td>\n",
       "      <td>-0.059384</td>\n",
       "      <td>0.021904</td>\n",
       "      <td>-0.046019</td>\n",
       "      <td>-0.067627</td>\n",
       "      <td>0.066743</td>\n",
       "      <td>-0.064045</td>\n",
       "      <td>0.041706</td>\n",
       "      <td>-0.014796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.036720</td>\n",
       "      <td>0.048992</td>\n",
       "      <td>-0.009070</td>\n",
       "      <td>0.100696</td>\n",
       "      <td>-0.034379</td>\n",
       "      <td>-0.002581</td>\n",
       "      <td>0.046219</td>\n",
       "      <td>-0.048808</td>\n",
       "      <td>0.087476</td>\n",
       "      <td>0.094753</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072960</td>\n",
       "      <td>0.044974</td>\n",
       "      <td>-0.114639</td>\n",
       "      <td>0.014727</td>\n",
       "      <td>-0.047793</td>\n",
       "      <td>-0.060343</td>\n",
       "      <td>0.034904</td>\n",
       "      <td>-0.066511</td>\n",
       "      <td>0.021394</td>\n",
       "      <td>-0.018467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.068221</td>\n",
       "      <td>0.031535</td>\n",
       "      <td>-0.022251</td>\n",
       "      <td>0.091936</td>\n",
       "      <td>-0.045524</td>\n",
       "      <td>0.062335</td>\n",
       "      <td>0.051493</td>\n",
       "      <td>-0.050584</td>\n",
       "      <td>0.091021</td>\n",
       "      <td>0.099479</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106683</td>\n",
       "      <td>0.033853</td>\n",
       "      <td>-0.139438</td>\n",
       "      <td>0.034197</td>\n",
       "      <td>-0.022073</td>\n",
       "      <td>-0.059132</td>\n",
       "      <td>-0.019083</td>\n",
       "      <td>-0.053927</td>\n",
       "      <td>0.019353</td>\n",
       "      <td>0.023621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.039528</td>\n",
       "      <td>0.033580</td>\n",
       "      <td>-0.006584</td>\n",
       "      <td>0.058266</td>\n",
       "      <td>-0.020921</td>\n",
       "      <td>0.010489</td>\n",
       "      <td>0.040179</td>\n",
       "      <td>-0.054073</td>\n",
       "      <td>0.070070</td>\n",
       "      <td>0.041623</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108452</td>\n",
       "      <td>0.044189</td>\n",
       "      <td>-0.087717</td>\n",
       "      <td>0.020076</td>\n",
       "      <td>-0.043566</td>\n",
       "      <td>-0.031580</td>\n",
       "      <td>0.023033</td>\n",
       "      <td>-0.041160</td>\n",
       "      <td>0.047247</td>\n",
       "      <td>-0.022942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.050317  0.053415  0.038032  0.076007 -0.053392 -0.002179  0.034439   \n",
       "1  0.066517  0.071847 -0.025412  0.046563 -0.047924  0.037785  0.033784   \n",
       "2  0.036720  0.048992 -0.009070  0.100696 -0.034379 -0.002581  0.046219   \n",
       "3  0.068221  0.031535 -0.022251  0.091936 -0.045524  0.062335  0.051493   \n",
       "4  0.039528  0.033580 -0.006584  0.058266 -0.020921  0.010489  0.040179   \n",
       "\n",
       "        7         8         9    ...       290       291       292       293  \\\n",
       "0 -0.057433  0.076233  0.097119  ... -0.065795  0.010038 -0.116087  0.017102   \n",
       "1 -0.072095  0.089296  0.087283  ... -0.133700  0.024485 -0.059384  0.021904   \n",
       "2 -0.048808  0.087476  0.094753  ... -0.072960  0.044974 -0.114639  0.014727   \n",
       "3 -0.050584  0.091021  0.099479  ... -0.106683  0.033853 -0.139438  0.034197   \n",
       "4 -0.054073  0.070070  0.041623  ... -0.108452  0.044189 -0.087717  0.020076   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0 -0.040221 -0.023342  0.002918 -0.075081  0.041535 -0.019835  \n",
       "1 -0.046019 -0.067627  0.066743 -0.064045  0.041706 -0.014796  \n",
       "2 -0.047793 -0.060343  0.034904 -0.066511  0.021394 -0.018467  \n",
       "3 -0.022073 -0.059132 -0.019083 -0.053927  0.019353  0.023621  \n",
       "4 -0.043566 -0.031580  0.023033 -0.041160  0.047247 -0.022942  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create embedding for each review\n",
    "import numpy as np\n",
    "def get_embedding(words):\n",
    "    embedding = [wv_pretrained[word] for word in words if word in wv_pretrained]\n",
    "    return np.mean(embedding, axis=0)\n",
    " \n",
    "embedding=df['cleaned_text'].apply(get_embedding)  \n",
    " \n",
    " \n",
    "data=pd.DataFrame(embedding.tolist())\n",
    "data.head()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Concatenate , drop a column\n",
    "df1=pd.concat([df,data],axis=1)\n",
    "df1.drop(columns=['cleaned_text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8511\n",
      "F1 Score: 0.8527055099416362\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score , f1_score \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df1['sentiment'] = le.fit_transform(df1['sentiment'])\n",
    "X = df1.drop(columns=['review','sentiment'])\n",
    "y = df1['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ##skip gram\n",
    "sentences = df['cleaned_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    sg=1,\n",
    "    vector_size=50,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'wonderful': [-1.1197125e-02  2.7722521e-02  5.1764075e-02 -2.2492981e-01\n",
      " -2.4227369e-01 -1.0925724e-01  5.4862648e-01  8.3991802e-01\n",
      " -1.0343041e+00  3.1746390e-01  3.5397848e-01 -2.3054768e-01\n",
      " -4.2786890e-01  6.5049849e-04  3.1856555e-01  4.6437845e-01\n",
      "  4.7221079e-01  4.8213974e-01 -5.4019642e-01 -5.5445784e-01\n",
      "  1.9933293e-02  1.8164086e-01  7.3803437e-01 -6.2802255e-01\n",
      "  3.3847305e-01  8.3403200e-01  1.3866916e-01 -2.6086724e-01\n",
      " -4.0437028e-01  3.1261167e-01  1.2566680e-01 -3.0601751e-02\n",
      "  1.9979376e-01  4.1377971e-01 -4.0805596e-01  1.6218163e-01\n",
      "  9.4526112e-01 -1.2563322e-01 -3.5124242e-01 -3.9210328e-01\n",
      "  4.5506012e-01 -2.2178869e-01 -6.7554349e-01  3.6265716e-01\n",
      "  4.2494902e-01 -1.9606239e-01 -1.4629734e-01 -7.1701878e-01\n",
      " -1.3865536e-01  2.9920844e-02]\n"
     ]
    }
   ],
   "source": [
    "word_vector = skipgram_model.wv['wonderful']\n",
    "print(f\"Vector for 'wonderful': {word_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_model.save(\"skipgram_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loaded_model = Word2Vec.load(\"skipgram_model.model\")\n",
    "loaded_model=loaded_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.103229</td>\n",
       "      <td>0.104894</td>\n",
       "      <td>-0.103497</td>\n",
       "      <td>0.015613</td>\n",
       "      <td>-0.149234</td>\n",
       "      <td>-0.213011</td>\n",
       "      <td>0.436361</td>\n",
       "      <td>0.576237</td>\n",
       "      <td>-0.670041</td>\n",
       "      <td>-0.164278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403076</td>\n",
       "      <td>-0.117490</td>\n",
       "      <td>-0.041954</td>\n",
       "      <td>0.011794</td>\n",
       "      <td>0.664964</td>\n",
       "      <td>0.283390</td>\n",
       "      <td>-0.444477</td>\n",
       "      <td>-0.258842</td>\n",
       "      <td>0.235374</td>\n",
       "      <td>0.362104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.118175</td>\n",
       "      <td>0.028888</td>\n",
       "      <td>-0.187211</td>\n",
       "      <td>0.067682</td>\n",
       "      <td>-0.122106</td>\n",
       "      <td>-0.165189</td>\n",
       "      <td>0.457229</td>\n",
       "      <td>0.566867</td>\n",
       "      <td>-0.619285</td>\n",
       "      <td>-0.039844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438902</td>\n",
       "      <td>-0.075632</td>\n",
       "      <td>-0.099666</td>\n",
       "      <td>0.131157</td>\n",
       "      <td>0.615313</td>\n",
       "      <td>0.320734</td>\n",
       "      <td>-0.363197</td>\n",
       "      <td>-0.346836</td>\n",
       "      <td>0.180858</td>\n",
       "      <td>0.295269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.097295</td>\n",
       "      <td>-0.015022</td>\n",
       "      <td>-0.060614</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>-0.147002</td>\n",
       "      <td>-0.219163</td>\n",
       "      <td>0.489888</td>\n",
       "      <td>0.673199</td>\n",
       "      <td>-0.708634</td>\n",
       "      <td>-0.103532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425551</td>\n",
       "      <td>-0.153646</td>\n",
       "      <td>-0.099040</td>\n",
       "      <td>0.126654</td>\n",
       "      <td>0.656246</td>\n",
       "      <td>0.221040</td>\n",
       "      <td>-0.447877</td>\n",
       "      <td>-0.376171</td>\n",
       "      <td>0.199675</td>\n",
       "      <td>0.376352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.197245</td>\n",
       "      <td>0.093371</td>\n",
       "      <td>-0.159989</td>\n",
       "      <td>0.083869</td>\n",
       "      <td>-0.099473</td>\n",
       "      <td>-0.232639</td>\n",
       "      <td>0.490801</td>\n",
       "      <td>0.559206</td>\n",
       "      <td>-0.698771</td>\n",
       "      <td>-0.178947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.436883</td>\n",
       "      <td>-0.245884</td>\n",
       "      <td>-0.080615</td>\n",
       "      <td>0.082951</td>\n",
       "      <td>0.686213</td>\n",
       "      <td>0.305047</td>\n",
       "      <td>-0.433893</td>\n",
       "      <td>-0.370824</td>\n",
       "      <td>0.275647</td>\n",
       "      <td>0.368934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.196511</td>\n",
       "      <td>0.036282</td>\n",
       "      <td>-0.034077</td>\n",
       "      <td>0.005689</td>\n",
       "      <td>-0.170700</td>\n",
       "      <td>-0.268549</td>\n",
       "      <td>0.479393</td>\n",
       "      <td>0.529781</td>\n",
       "      <td>-0.709934</td>\n",
       "      <td>-0.058372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424220</td>\n",
       "      <td>-0.174459</td>\n",
       "      <td>-0.084798</td>\n",
       "      <td>0.058803</td>\n",
       "      <td>0.622099</td>\n",
       "      <td>0.281144</td>\n",
       "      <td>-0.368045</td>\n",
       "      <td>-0.291468</td>\n",
       "      <td>0.101870</td>\n",
       "      <td>0.295844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.103229  0.104894 -0.103497  0.015613 -0.149234 -0.213011  0.436361   \n",
       "1 -0.118175  0.028888 -0.187211  0.067682 -0.122106 -0.165189  0.457229   \n",
       "2 -0.097295 -0.015022 -0.060614  0.003673 -0.147002 -0.219163  0.489888   \n",
       "3 -0.197245  0.093371 -0.159989  0.083869 -0.099473 -0.232639  0.490801   \n",
       "4 -0.196511  0.036282 -0.034077  0.005689 -0.170700 -0.268549  0.479393   \n",
       "\n",
       "         7         8         9   ...        40        41        42        43  \\\n",
       "0  0.576237 -0.670041 -0.164278  ...  0.403076 -0.117490 -0.041954  0.011794   \n",
       "1  0.566867 -0.619285 -0.039844  ...  0.438902 -0.075632 -0.099666  0.131157   \n",
       "2  0.673199 -0.708634 -0.103532  ...  0.425551 -0.153646 -0.099040  0.126654   \n",
       "3  0.559206 -0.698771 -0.178947  ...  0.436883 -0.245884 -0.080615  0.082951   \n",
       "4  0.529781 -0.709934 -0.058372  ...  0.424220 -0.174459 -0.084798  0.058803   \n",
       "\n",
       "         44        45        46        47        48        49  \n",
       "0  0.664964  0.283390 -0.444477 -0.258842  0.235374  0.362104  \n",
       "1  0.615313  0.320734 -0.363197 -0.346836  0.180858  0.295269  \n",
       "2  0.656246  0.221040 -0.447877 -0.376171  0.199675  0.376352  \n",
       "3  0.686213  0.305047 -0.433893 -0.370824  0.275647  0.368934  \n",
       "4  0.622099  0.281144 -0.368045 -0.291468  0.101870  0.295844  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def skip_embedding(words):\n",
    "    embedding = [loaded_model[word] for word in words if word in loaded_model]\n",
    "    return np.mean(embedding, axis=0)\n",
    " \n",
    "embedding = df['cleaned_text'].apply(skip_embedding)  \n",
    " \n",
    " \n",
    "data=pd.DataFrame(embedding.tolist())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Concatenate , drop a column\n",
    "df2=pd.concat([df,data],axis=1)\n",
    "df2.drop(columns=['cleaned_text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8647\n",
      "F1 Score: 0.8660263392415091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score , f1_score \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df2['sentiment'] = le.fit_transform(df2['sentiment'])\n",
    "X = df2.drop(columns=['review','sentiment'])\n",
    "y = df2['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CBOW \n",
    "cbow_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    sg=0,\n",
    "    vector_size=50,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model.save(\"cbow_model.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model = Word2Vec.load(\"cbow_model.model\")\n",
    "loaded_model=cbow_model.wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'wonderful': [-0.70591295  0.6509652  -2.5713575  -3.7275262   0.21396323 -1.0881749\n",
      "  0.6108621   2.440779   -0.03457995 -0.8260493  -1.9220332  -0.7386559\n",
      " -2.6185114   0.16978963  0.33179563  0.9136808   0.12419671  1.4620786\n",
      " -1.4964484  -1.5296903  -0.89964825 -0.775082    2.509701   -2.8479114\n",
      " -0.98070556  2.1681745   1.9152339   1.9516232   1.1545634   1.1230468\n",
      "  0.9183077   0.16525657 -1.5275582   3.246119   -0.999611    0.3915947\n",
      "  2.5906413  -0.93570554  1.7606449  -0.45751682  2.7845728   1.250187\n",
      " -3.8893487   2.0935001  -0.29709792 -3.2667446   1.729668   -1.3144089\n",
      " -2.2435625  -3.0615244 ]\n"
     ]
    }
   ],
   "source": [
    "word_vector = cbow_model.wv['wonderful']\n",
    "print(f\"Vector for 'wonderful': {word_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.368710</td>\n",
       "      <td>-0.313098</td>\n",
       "      <td>0.092859</td>\n",
       "      <td>-0.086129</td>\n",
       "      <td>-0.401625</td>\n",
       "      <td>0.222938</td>\n",
       "      <td>0.166105</td>\n",
       "      <td>0.912390</td>\n",
       "      <td>-0.812800</td>\n",
       "      <td>-0.431937</td>\n",
       "      <td>...</td>\n",
       "      <td>0.876117</td>\n",
       "      <td>-0.053534</td>\n",
       "      <td>0.181403</td>\n",
       "      <td>-0.272961</td>\n",
       "      <td>1.193235</td>\n",
       "      <td>-0.552502</td>\n",
       "      <td>0.084231</td>\n",
       "      <td>-0.254845</td>\n",
       "      <td>-0.012255</td>\n",
       "      <td>0.444466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.567885</td>\n",
       "      <td>-0.224408</td>\n",
       "      <td>-0.655002</td>\n",
       "      <td>-0.414184</td>\n",
       "      <td>-0.347915</td>\n",
       "      <td>0.330098</td>\n",
       "      <td>0.141622</td>\n",
       "      <td>0.830752</td>\n",
       "      <td>-0.471493</td>\n",
       "      <td>-0.501901</td>\n",
       "      <td>...</td>\n",
       "      <td>1.834009</td>\n",
       "      <td>0.224204</td>\n",
       "      <td>-0.281282</td>\n",
       "      <td>0.076449</td>\n",
       "      <td>0.767704</td>\n",
       "      <td>-0.250526</td>\n",
       "      <td>0.632132</td>\n",
       "      <td>-0.626143</td>\n",
       "      <td>-0.457365</td>\n",
       "      <td>0.122881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.409077</td>\n",
       "      <td>-0.492577</td>\n",
       "      <td>-0.115155</td>\n",
       "      <td>-0.357869</td>\n",
       "      <td>-0.403937</td>\n",
       "      <td>0.274879</td>\n",
       "      <td>0.324552</td>\n",
       "      <td>1.184395</td>\n",
       "      <td>-0.942448</td>\n",
       "      <td>-0.660638</td>\n",
       "      <td>...</td>\n",
       "      <td>1.208927</td>\n",
       "      <td>0.046204</td>\n",
       "      <td>0.329998</td>\n",
       "      <td>0.024962</td>\n",
       "      <td>0.848576</td>\n",
       "      <td>-0.955530</td>\n",
       "      <td>-0.049832</td>\n",
       "      <td>-0.707158</td>\n",
       "      <td>-0.422876</td>\n",
       "      <td>-0.026675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.305401</td>\n",
       "      <td>-0.107770</td>\n",
       "      <td>-0.134137</td>\n",
       "      <td>-0.215079</td>\n",
       "      <td>-0.113557</td>\n",
       "      <td>0.012237</td>\n",
       "      <td>0.331398</td>\n",
       "      <td>0.765705</td>\n",
       "      <td>-0.886531</td>\n",
       "      <td>-0.464718</td>\n",
       "      <td>...</td>\n",
       "      <td>1.445303</td>\n",
       "      <td>-0.340621</td>\n",
       "      <td>0.335324</td>\n",
       "      <td>-0.229817</td>\n",
       "      <td>1.388782</td>\n",
       "      <td>-0.835881</td>\n",
       "      <td>0.365231</td>\n",
       "      <td>-0.777140</td>\n",
       "      <td>-0.387788</td>\n",
       "      <td>0.110122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.517093</td>\n",
       "      <td>-0.210835</td>\n",
       "      <td>0.028965</td>\n",
       "      <td>-0.474007</td>\n",
       "      <td>-0.461286</td>\n",
       "      <td>-0.113311</td>\n",
       "      <td>-0.021774</td>\n",
       "      <td>0.677775</td>\n",
       "      <td>-0.643828</td>\n",
       "      <td>-0.468360</td>\n",
       "      <td>...</td>\n",
       "      <td>1.564413</td>\n",
       "      <td>0.176379</td>\n",
       "      <td>0.040755</td>\n",
       "      <td>-0.185483</td>\n",
       "      <td>0.998282</td>\n",
       "      <td>-0.319978</td>\n",
       "      <td>0.772152</td>\n",
       "      <td>-0.622290</td>\n",
       "      <td>-0.287472</td>\n",
       "      <td>0.247715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.368710 -0.313098  0.092859 -0.086129 -0.401625  0.222938  0.166105   \n",
       "1 -0.567885 -0.224408 -0.655002 -0.414184 -0.347915  0.330098  0.141622   \n",
       "2 -0.409077 -0.492577 -0.115155 -0.357869 -0.403937  0.274879  0.324552   \n",
       "3 -0.305401 -0.107770 -0.134137 -0.215079 -0.113557  0.012237  0.331398   \n",
       "4 -0.517093 -0.210835  0.028965 -0.474007 -0.461286 -0.113311 -0.021774   \n",
       "\n",
       "         7         8         9   ...        40        41        42        43  \\\n",
       "0  0.912390 -0.812800 -0.431937  ...  0.876117 -0.053534  0.181403 -0.272961   \n",
       "1  0.830752 -0.471493 -0.501901  ...  1.834009  0.224204 -0.281282  0.076449   \n",
       "2  1.184395 -0.942448 -0.660638  ...  1.208927  0.046204  0.329998  0.024962   \n",
       "3  0.765705 -0.886531 -0.464718  ...  1.445303 -0.340621  0.335324 -0.229817   \n",
       "4  0.677775 -0.643828 -0.468360  ...  1.564413  0.176379  0.040755 -0.185483   \n",
       "\n",
       "         44        45        46        47        48        49  \n",
       "0  1.193235 -0.552502  0.084231 -0.254845 -0.012255  0.444466  \n",
       "1  0.767704 -0.250526  0.632132 -0.626143 -0.457365  0.122881  \n",
       "2  0.848576 -0.955530 -0.049832 -0.707158 -0.422876 -0.026675  \n",
       "3  1.388782 -0.835881  0.365231 -0.777140 -0.387788  0.110122  \n",
       "4  0.998282 -0.319978  0.772152 -0.622290 -0.287472  0.247715  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def cbow_embedding(words):\n",
    "    embedding = [loaded_model[word] for word in words if word in loaded_model]\n",
    "    return np.mean(embedding, axis=0)\n",
    " \n",
    "embedding = df['cleaned_text'].apply(cbow_embedding)  \n",
    " \n",
    " \n",
    "data=pd.DataFrame(embedding.tolist())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Concatenate , drop a column\n",
    "df3=pd.concat([df,data],axis=1)\n",
    "df3.drop(columns=['cleaned_text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8449\n",
      "F1 Score: 0.8465723612622416\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score , f1_score \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df3['sentiment'] = le.fit_transform(df3['sentiment'])\n",
    "X = df3.drop(columns=['review','sentiment'])\n",
    "y = df3['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
